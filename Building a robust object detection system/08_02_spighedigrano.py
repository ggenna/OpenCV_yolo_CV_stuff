import os
import cv2
import numpy as np
import pandas as pd
import torch
from torch.utils.data import Dataset
from PIL import Image
from torchvision import transforms
from torch.utils.data import DataLoader
import sys
sys.path.append('../../yolov5_clean')  

from models.common import DetectMultiBackend

from utils.general import non_max_suppression
import torchvision


# Carica il CSV maturity_train.csv che contiene:
# image_name: nome file immagine
# BoxesString: coordinate bounding box (es: 99 692 160 764;641 27 697 115;...)
# maturity_label: etichette maturit√† (es: young;mature;overripe;...)

class WheatDatasetWithStages(Dataset):
    def __init__(self, annotations_file, img_dir, transform=None):
        self.annotations = pd.read_csv(annotations_file)
        self.img_dir = img_dir
        self.transform = transform

    def __len__(self):
        return len(self.annotations)
    
    def __getitem__(self, idx):
        img_path = os.path.join(self.img_dir, self.annotations.iloc[idx, 0])
        image = Image.open(img_path).convert("RGB")

        #Legge le stringhe di box e label dal CSV
        boxes_str = self.annotations.iloc[idx, 1]
        labels_str = self.annotations.iloc[idx, 2]
        label_map = {'young': 0, 'mature': 1, 'overripe': 2}

        #Se non ci sono box, crea tensori vuoti
        if boxes_str.strip().lower() == "no_box":
            boxes = torch.zeros((0, 4), dtype=torch.float32)
            maturity_labels = torch.zeros((0,), dtype=torch.int64)
        else:
            # Converte le stringhe in liste:
            #     "99 692 160 764;641 27 697 115" ‚Üí [[99, 692, 160, 764], [641, 27, 697, 115]]
            #     "young;mature" ‚Üí [0, 1] (usando label_map)
            boxes = [list(map(float, box.split())) for box in boxes_str.split(';')]
            maturity_labels = [label_map[label.strip()] for label in labels_str.split(';')]
            boxes = torch.tensor(boxes, dtype=torch.float32)
            maturity_labels = torch.tensor(maturity_labels, dtype=torch.int64)

        if self.transform:
            image = self.transform(image)
        return image, boxes, maturity_labels
        # Risultato:
        # image: Tensor [3, H, W]
        # boxes: Tensor [[99, 692, 160, 764], [641, 27, 697, 115], ...]
        # maturity_labels: Tensor [0, 1, 2, 0, ...]  # 0=young, 1=mature, 2=overripe

transform = transforms.ToTensor()

#Carica Dataset e Test
csv_path = '../gwhd_2021/maturity_train.csv'
img_dir = '../gwhd_2021/images'

dataset = WheatDatasetWithStages(csv_path, img_dir, transform=transform)
from torch.utils.data import Subset
dataset = Subset(dataset, list(range(250)))
#Test del dataset:
image, boxes, labels = dataset[0]

print("Image shape:", image.shape)
print("Boxes:", boxes)
print("Maturity labels:", labels)

#Modifica YOLOv5 per Classificazione Maturit√†
#DetectMultiBackend √® una classe wrapper di YOLOv5 che permette di caricare modelli YOLO da diverse fonti e formati in modo unificato.
model = DetectMultiBackend(weights='yolov5s.pt', device='cpu')
detect_layer = model.model.model[-1]  

# Carica YOLOv5 pre-addestrato e accede al layer di detection
# Modifica il numero di classi:
# Prima: 80 classi COCO (person, car, dog, ecc.)
# Dopo: 3 classi (young, mature, overripe)
num_maturity_classes = 3
detect_layer.nc = num_maturity_classes
detect_layer.no = num_maturity_classes + 5  
detect_layer.stride = torch.tensor([8., 16., 32.]) 
detect_layer.anchor_grid = [torch.zeros(1)] * len(detect_layer.anchor_grid)  # reset anchor grid

#Ricrea i layer di output per adattarli alle nuove 3 classi
for i, m in enumerate(detect_layer.m):
    detect_layer.m[i] = torch.nn.Conv2d(
        in_channels=m.in_channels,
        out_channels=detect_layer.no * len(detect_layer.anchors[i]),
        kernel_size=m.kernel_size,
        stride=m.stride,
        padding=m.padding
    )

print("‚úÖ YOLOv5 head modified for maturity classification.")

# Training Loop (Semplificato)
# Setup:
#     DataLoader: carica batch di 2 immagini
#     Adam: ottimizzatore
#     MSELoss: loss function (‚ö†Ô∏è non ideale per detection, √® solo demo)
train_loader = DataLoader(dataset, batch_size=2, shuffle=True, collate_fn=lambda x: tuple(zip(*x)))
optimizer = torch.optim.Adam(model.parameters(), lr=0.0001)
criterion = torch.nn.MSELoss()

#  Nota: Questo √® un training semplificato/demo. Usa target dummy (tutti zeri) solo per mostrare il flusso. Un training reale userebbe:

# Loss YOLO (bbox + classification + confidence)
# Ground truth veri
# Pi√π epoche

for epoch in range(2):
    model.train()
    for i, (images, boxes, maturity_labels) in enumerate(train_loader):
        images = torch.stack(images) 
        dummy_outputs = model(images) 
        target = torch.zeros_like(dummy_outputs[0]) if isinstance(dummy_outputs, (list, tuple)) else torch.zeros_like(dummy_outputs)
        loss = criterion(dummy_outputs[0], target) if isinstance(dummy_outputs, (list, tuple)) else criterion(dummy_outputs, target)
        optimizer.zero_grad()
        loss.backward()
        optimizer.step()

        print(f"Epoch {epoch} - Step {i} - Loss: {loss.item():.4f}")

# Data Augmentation (Condizioni Difficili)
# Crea varianti dell'immagine:
#     Immagine sfocata (simula cattiva qualit√†)
#     Immagine scura (simula scarsa illuminazione)
def apply_challenging_conditions(image):
    blurred = cv2.GaussianBlur(image, (5, 5), 0) ## Sfocatura
    darker = cv2.convertScaleAbs(image, alpha=0.5, beta=0)  # Riduce luminosit√†
    return [blurred, darker]

print("seleziono la prima immagine [0,0]",dataset.dataset.annotations.iloc[0, 0])
pil_image = Image.open(dataset.dataset.img_dir + '/' + dataset.dataset.annotations.iloc[0, 0]).convert("RGB")
cv_image = np.array(pil_image)[:, :, ::-1].copy()  
challenged_images = apply_challenging_conditions(cv_image)

# Inferenza e Visualizzazione
def draw_and_save_predictions(img_tensor, pred, output_path, names):
    img = img_tensor.squeeze().permute(1, 2, 0).numpy() * 255
    img = img.astype(np.uint8).copy()

    #Converte tensor in immagine numpy
    if pred is not None and len(pred):
        for *xyxy, conf, cls in pred:
            label = f"{names[int(cls)]} {conf:.2f}"
            xyxy = [int(x.item()) for x in xyxy]
            cv2.rectangle(img, (xyxy[0], xyxy[1]), (xyxy[2], xyxy[3]), (0, 255, 0), 2)
            cv2.putText(img, label, (xyxy[0], xyxy[1] - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.6, (0, 255, 0), 2)
#  Per ogni detection:
#     Disegna rettangolo verde
#     Aggiunge testo: "mature 0.85" (classe + confidence)
    cv2.imwrite(output_path, cv2.cvtColor(img, cv2.COLOR_RGB2BGR))
    print(f"‚úÖ Saved: {output_path}")

#Test su Immagini con Condizioni Difficili
maturity_names = ['young', 'mature', 'overripe']
model.eval()

for idx, img in enumerate([np.array(pil_image)] + challenged_images):
    rgb_img = img[:, :, ::-1] if img.shape[-1] == 3 else img
    pil_ver = Image.fromarray(rgb_img)
    input_tensor = transforms.ToTensor()(pil_ver).unsqueeze(0)
# Prepara 3 varianti:
#     Immagine originale
#     Immagine sfocata
#     Immagine scura
    with torch.no_grad():
        pred = model(input_tensor)
        pred = non_max_suppression(pred, conf_thres=0.3, iou_thres=0.45)[0]

        draw_and_save_predictions(input_tensor, pred, f"output_{idx}.png", maturity_names)

# ## üéØ **RISULTATO FINALE**

# Il codice produce **3 immagini annotate:**
# ```
# output_0.png  ‚Üí  Immagine originale con box verdi:
#                  [mature 0.87] [young 0.72] [overripe 0.65] ...

# output_1.png  ‚Üí  Immagine SFOCATA con predizioni
#                  (testa robustezza a blur)

# output_2.png  ‚Üí  Immagine SCURA con predizioni
#                  (testa robustezza a scarsa luce)
# ```

# ---

# ## üìä **FLUSSO COMPLETO VISUALIZZATO**
# ```
# CSV + Images
#      ‚Üì
# [Dataset] ‚Üí Carica immagini + box + label
#      ‚Üì
# [YOLOv5] ‚Üí Modifica per 3 classi (young/mature/overripe)
#      ‚Üì
# [Training] ‚Üí Addestra su 250 immagini (2 epoche)
#      ‚Üì
# [Augmentation] ‚Üí Crea varianti (blur, dark)
#      ‚Üì
# [Inference] ‚Üí Predice su 3 varianti
#      ‚Üì
# [Output] ‚Üí Salva 3 immagini annotate con box colorati